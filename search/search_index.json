{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>You are about to embark on a transformative journey as we explore the integration of AI into enterprise applications during this workshop.  With technology evolving at an unprecedented pace, this workshop aims to demystify the process of infusing Artificial Intelligence and Large Language Models (LLM) into Quarkus applications.</p> <p></p> Overall overview of the workshop"},{"location":"#once-upon-a-time-quarkus","title":"Once upon a time, Quarkus","text":"<p>Quarkus was released in March 2019 by a team of engineers with the dream of supercharging Java for Cloud/Kubernetes deployments while bringing joy back to developers.  Over the past decades, Java has dominated enterprise development, being used to build and run some of the most critical applications in the world.  However, with the rise of Cloud and Kubernetes as preferred deployment platforms, Java struggled to keep up with the new requirements of Cloud Native development.  Quarkus was created to bridge this gap.</p> <p>If you visit the Quarkus website, you\u2019ll find that Quarkus is \u201cA Kubernetes Native Java stack tailored for OpenJDK HotSpot &amp; GraalVM, crafted from the best of breed Java libraries and standards.\u201d  This somewhat cryptic description does an outstanding job at using bankable keywords.  Additionally, it\u2019s proudly proclaimed: \u201cSupersonic Subatomic Java,\u201d signifying that Quarkus is exceptionally fast and lightweight.  But what exactly does Quarkus do?</p> <p>In practice, Quarkus is a stack designed for developing distributed systems and modern applications in Java or Kotlin.  Quarkus applications are optimized for the Cloud, containers, and Kubernetes.  While Quarkus can be used in other environments, the principles infused in Quarkus have made containerization of applications more efficient.</p> <p>In this workshop, you will learn how to use Quarkus to implement and execute enterprise Java applications interacting with LLMs.</p>"},{"location":"#when-quarkus-meets-ai","title":"When Quarkus meets AI","text":"<p>AI is reshaping the software landscape, influencing user interactions and the foundations of software development.  However, AI and Java haven\u2019t always been the best of friends.</p> <p>The Quarkus team has been contemplating this challenge for a while.  They\u2019ve been figuring out how to make AI and Java work together, but not just any Java: Quarkus Java.</p> <p>Integrating AI can be approached in various ways, but properly integrating AI into an enterprise application is not trivial.  There are numerous challenges, including:</p> <ul> <li>How to integrate AI into an application without leaking concepts everywhere?</li> <li>How to integrate multiple models without having to deal with many different APIs</li> <li>How to make this AI integration observable?</li> <li>What about auditing?</li> <li>How to provide data to the AI to improve prediction accuracy?</li> </ul> <p>In this workshop, you will learn how to integrate AI into a Quarkus application. We are going to use Quarkus Langchain4J, a Quarkus extension that provides a unified API to interact with multiple AI models (based on langchain4j)</p> <p> </p> Integrating multiple LLMs models Note <p>This workshop is not about teaching you how to use AI.  Instead, it\u2019s about how to integrate AI into a Quarkus application.</p> Note <p>To learn more about Quarkus and LLM integration, you can refer to the Quarkus meets LangChain4j article.</p>"},{"location":"#what-are-you-going-to-learn","title":"What are you going to learn?","text":"<p>This workshop is divided into four parts.</p> <ul> <li> <p>Slot 1: Unveiling the Potential - Integrating OpenAI with Quarkus</p> </li> <li> <p>Experience the hype: Learn to integrate OpenAI into a Quarkus application seamlessly.</p> </li> <li>Designing Interactions: Explore strategies for crafting meaningful interactions with large language models.</li> <li> <p>AI Empowerment: Discover tools and techniques for providing robust control over AI within your application.</p> </li> <li> <p>Slot 2: Augmented Generation - Extending Hosted LLMs with Local Documents</p> </li> <li> <p>Shift gears: Explore another LLM and extend its capabilities using local documents.</p> </li> <li> <p>Retrieval Augmented Generation: Delve into one of the core patterns in AI usage, understanding the synergy between your data and LLM interactions.</p> </li> <li> <p>Slots 3 &amp; 4: OpenShift AI - Training and Hosting Your Own Model</p> </li> <li> <p>Harness the power of OpenShift AI and its MLOps capabilities: Learn how it facilitates the training and hosting of custom AI models.</p> </li> <li>Practical Integration: Witness the integration of a custom model into a Quarkus application to complete the end-to-end narrative.</li> </ul>"},{"location":"#prerequisites","title":"Prerequisites","text":"<p>This workshop aims to be as self-explanatory as possible.  Follow the instructions, perform the tasks, and feel free to ask for clarification or assistance\u2014our team is here to help.</p> <p>Firstly, ensure you have a 64-bit computer with admin rights (for installing necessary tools) and at least 8GB of RAM.</p> <p>This workshop will use the following software, tools, frameworks that you need to install and understand:</p> <ul> <li>Any IDE you feel comfortable with (e.g., Intellij IDEA, Eclipse IDE, VS Code).</li> <li>JDK 17.0.9.</li> <li>Docker or Podman (see instructions in the appendix).</li> <li>cURL (should be installed; if not, check the appendix).</li> <li>Ollama and the Llama 2 model (see instructions in the appendix).</li> <li>An Azure OpenAI key (see instructions in the appendix).</li> </ul> <p>We will also use Maven 3.9.6, but there\u2019s no need to install it. The workshop scaffolding includes a maven wrapper, <code>mvnw</code>.</p> <p>Info</p> <p>This workshop assumes a bash shell. If you run on Windows, particularly, adjust the commands accordingly or install WSL. See the appendix for installation instructions.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Once you have installed all the prerequisites, you can start the workshop.  Download and unzip the workshop archive, open a terminal in the <code>quarkus-llm-workshop</code> directory, and run the following command to start the workshop:</p> <pre><code>$ ./mvnw verify\n</code></pre> <p>This command will build the code of the workshop.  During the workshop, we will explore each part of the code, explaining how it works and demonstrating how to run the various applications.</p> <p>It\u2019s time to witness some code in action!</p>"},{"location":"appendixes/accessing-azureai/","title":"Accessing Azure OpenAI","text":"<p>If you already have an Azure subscription, you can use it to create an Azure OpenAI resource. Otherwise, you can create a free Azure subscription and use it to create an Azure OpenAI resource. Then, you need to use the Azure OpenAI service to generate the API key and endpoint URL. You can either do that using the Azure Portal or the Azure CLI.</p> <p>The easiest being use the Azure CLI, we recommend that you install it and use it to create the Azure OpenAI resource and generate the API key and endpoint URL. Once Azure CLI is installed and you have your Azure subscription, sign in to your Azure account:</p> <pre><code>az login\n</code></pre> Check Azure subscription <p>If you have several Azure subscription, make sure you are using the right one. For that, you can execute the following command to list all your subscriptions and set the one you want to use:</p> <pre><code>az account list --output table\naz account set --subscription &lt;subscription-id&gt;\naz account show\n</code></pre> <p>Then, execute the following command to create the Azure OpenAI resources:</p> <pre><code>echo \"Setting up environment variables...\"\necho \"----------------------------------\"\nPROJECT=\"&lt;give-your-project-a-name&gt;\"\nRESOURCE_GROUP=\"rg-$PROJECT\"\nLOCATION=\"eastus\"\nTAG=\"$PROJECT\"\nAI_SERVICE=\"ai-$PROJECT\"\nAI_DEPLOYMENT=\"gpt35turbo\"\n\necho \"Creating the resource group...\"\necho \"------------------------------\"\naz group create \\\n  --name \"$RESOURCE_GROUP\" \\\n  --location \"$LOCATION\" \\\n  --tags system=\"$TAG\"\n\necho \"Creating the Cognitive Service...\"\necho \"---------------------------------\"\naz cognitiveservices account create \\\n  --name \"$AI_SERVICE\" \\\n  --resource-group \"$RESOURCE_GROUP\" \\\n  --location \"$LOCATION\" \\\n  --custom-domain \"$AI_SERVICE\" \\\n  --tags system=\"$TAG\" \\\n  --kind \"OpenAI\" \\\n  --sku \"S0\"\n\necho \"Deploying the model...\"\necho \"----------------------\"\naz cognitiveservices account deployment create \\\n  --name \"$AI_SERVICE\" \\\n  --resource-group \"$RESOURCE_GROUP\" \\\n  --deployment-name \"$AI_DEPLOYMENT\" \\\n  --model-name \"gpt-35-turbo\" \\\n  --model-version \"0301\"  \\\n  --model-format \"OpenAI\" \\\n  --sku-capacity 1 \\\n  --sku-name \"Standard\"\n</code></pre> <p>Once the script has executed, you can use the following command to get the API key and endpoint URL. You will need these properties later one when you will configure the application.</p> <pre><code>echo \"Storing the key and endpoint in environment variables...\"\necho \"--------------------------------------------------------\"\nAZURE_OPENAI_KEY=$(\n  az cognitiveservices account keys list \\\n    --name \"$AI_SERVICE\" \\\n    --resource-group \"$RESOURCE_GROUP\" \\\n    | jq -r .key1\n  )\nAZURE_OPENAI_ENDPOINT=$(\n  az cognitiveservices account show \\\n    --name \"$AI_SERVICE\" \\\n    --resource-group \"$RESOURCE_GROUP\" \\\n    | jq -r .properties.endpoint\n  )\n\n# Set the properties\necho \"--------------------------------------------------\"\necho \"The following properties can be copied to either the application.properties:\"\necho \"--------------------------------------------------\"\necho \"quarkus.langchain4j.azure-openai.api-key=$AZURE_OPENAI_KEY\"\necho \"quarkus.langchain4j.azure-openai.deployment-id=$AI_DEPLOYMENT\"\necho \"quarkus.langchain4j.azure-openai.resource-name=$AI_SERVICE\"\n</code></pre> <p>Once you\u2019ve finished the workshop, remember to delete the Azure OpenAI resources to avoid being charged for it:</p> <pre><code># Clean up\naz group delete \\\n  --name \"$RESOURCE_GROUP\" \\\n  --yes\n\naz cognitiveservices account purge \\\n  --name \"$AI_SERVICE\" \\\n  --resource-group \"$RESOURCE_GROUP\" \\\n  --location \"$LOCATION\"\n\naz cognitiveservices account delete \\\n  --name \"$AI_SERVICE\" \\\n  --resource-group \"$RESOURCE_GROUP\"\n</code></pre>"},{"location":"appendixes/installing-curl/","title":"Installing cURL","text":"<p>To invoke the REST Web Services described in this workshop, we often use cURL.<sup>1</sup> cURL is a command-line tool and library to do reliable data transfers with various protocols, including HTTP. It is free, open-source (available under the MIT Licence), and has been ported to several operating systems. If your operating system does not already include cURL (most do), here is how to install it.</p>"},{"location":"appendixes/installing-curl/#installing-curl_1","title":"Installing cURL","text":"<p>If you are on Mac OS X and have installed Homebrew, then installing cURL is just a matter of a single command.<sup>2</sup> Open your terminal and install cURL with the following command:</p> <pre><code>brew install curl\n</code></pre> <p>For Windows, download and install curl from the official website.</p>"},{"location":"appendixes/installing-curl/#checking-for-curl-installation","title":"Checking for cURL Installation","text":"<p>Once installed, check for cURL by running curl \u2013version in the terminal. It should display cURL version:</p> <pre><code>$ curl --version\ncurl 7.64.1 (x86_64-apple-darwin20.0) libcurl/7.64.1 (SecureTransport) LibreSSL/2.8.3 zlib/1.2.11 nghttp2/1.41.0\nRelease-Date: 2019-03-27\nProtocols: dict file ftp ftps gopher http https imap imaps ldap ldaps pop3 pop3s rtsp smb smbs smtp smtps telnet tftp\nFeatures: AsynchDNS GSS-API HTTP2 HTTPS-proxy IPv6 Kerberos Largefile libz MultiSSL NTLM NTLM_WB SPNEGO SSL UnixSockets\n</code></pre>"},{"location":"appendixes/installing-curl/#some-curl-commands","title":"Some cURL Commands","text":"<p>cURL is a command-line utility where you can use several parameters and options to invoke URLs. You invoke curl with zero, one, or several command-line options to accompany the URL (or set of URLs) you want the transfer to be about. cURL supports over two hundred different options, and I would recommend reading the documentation for more help<sup>3</sup>. To get some help on the commands and options, you can type, use the following command:</p> <pre><code>$ curl --help\n</code></pre> <p>You can also opt to use <code>curl --manual</code>, which will output the entire man page for cURL plus an appended tutorial for the most common use cases.</p> <p>Here are some commands you will use to invoke RESTful web services.</p> <ul> <li><code>curl http://localhost:8083/api/heroes/hello</code>: HTTP GET on a given URL.</li> <li><code>curl -X GET http://localhost:8083/api/heroes/hello</code>: Same effect as the previous command, an HTTP GET on a given URL.</li> <li><code>curl -v http://localhost:8083/api/heroes/hello</code>: HTTP GET on a given URL with verbose mode on.</li> <li><code>curl -H 'Content-Type: application/json' http://localhost:8083/api/heroes/hello</code>: HTTP GET on a given URL passing the JSON Content Type in the HTTP Header.</li> <li><code>curl -X DELETE http://localhost:8083/api/heroes/1</code>: HTTP DELETE on a given URL.</li> </ul>"},{"location":"appendixes/installing-curl/#references","title":"References","text":"<ol> <li> <p>cURL \u21a9</p> </li> <li> <p>Installing cURL on Mac OS X \u21a9</p> </li> <li> <p>cURL commands \u21a9</p> </li> </ol>"},{"location":"appendixes/installing-docker/","title":"Installing Docker or Podman","text":"<p>Docker is a set of utilities that use OS-level virtualization to deliver software in packages called containers. Containers are isolated from one another and bundle their software, libraries, and configuration files; they can communicate with each other through well-defined channels.</p>"},{"location":"appendixes/installing-docker/#installing-docker","title":"Installing Docker","text":"<p>Quarkus uses Testcontainers, and therefore also Docker, to ease the management of different technical services (database, monitoring\u2026) during development. Our workshop also uses Docker to manage these services in a production-style deployment. So for this, we need to install <code>docker</code> and <code>docker compose</code>. Installation instructions are available on the following page:</p> <ul> <li>Mac OS X - Docker for Mac (version 20+)</li> <li>Windows - Docker for Windows (version 20+)</li> <li>CentOS - Docker CE for CentOS</li> <li>Debian - Docker CE for Debian</li> <li>Fedora - Docker CE for Fedora</li> <li>Ubuntu - Docker CE for Ubuntu</li> </ul> <p>On Linux, don\u2019t forget the post-execution steps described on Linux Post-installation Steps.</p> <p>NOTE: If you do not have a Docker license, you might prefer <code>podman</code> (Podman) instead of <code>docker</code>. To install <code>podman</code> and <code>podman-compose</code>, please follow the instructions at Quarkus Podman Guide. Do not forget the extra steps to configure the Testcontainers library. As a convenience, you can even alias <code>docker</code> to <code>podman</code>.</p>"},{"location":"appendixes/installing-docker/#checking-for-docker-installation","title":"Checking for Docker Installation","text":"<p>Once installed, check that both <code>docker</code> and <code>docker compose</code> are available in your <code>PATH</code>. For that, execute the following commands:</p> <pre><code>$ docker version\n</code></pre> <p>You should see something like this:</p> <pre><code>Docker version 20.10.8, build 3967b7d\nCloud integration: v1.0.24\nVersion:           20.10.14\nAPI version:       1.41\n</code></pre> <p>Then, check the Docker Compose version:</p> <pre><code>$ docker compose version\n</code></pre> <p>Docker Compose being a separate utility, you should get a different version than Docker itself:</p> <pre><code>Docker Compose version v2.5.0\n</code></pre> <p>Finally, run your first container as follows:</p> <pre><code>$ docker run hello-world\n</code></pre> <p>For the first time, this will download the hello-world container image from the Docker Hub and run it.  You should get something like this:</p> <pre><code>Hello from Docker!\nThis message shows that your installation appears to be working correctly.\n\nTo generate this message, Docker took the following steps:\n1. The Docker client contacted the Docker daemon.\n2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub. (amd64)\n3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading.\n4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal.\n</code></pre> <p>To try something more ambitious, you can run an Ubuntu container with:</p> <pre><code>$ docker run -it ubuntu bash\n</code></pre>"},{"location":"appendixes/installing-docker/#some-docker-commands","title":"Some Docker Commands","text":"<p>Docker is a command-line utility where you can use several parameters and options to start/stop a container.  You invoke docker with zero, one, or several command-line options with the container or image ID you want to work with.  Docker comes with several options that are described in the documentation if you need more help<sup>1</sup>. To get some help on the commands and options, you can type, use the following command:</p> <pre><code>$ docker help\n</code></pre> <p>Here are some commands that you will be using to start/stop containers in this workshop.</p> <ul> <li><code>docker container ls</code>: Lists containers.</li> <li><code>docker container start CONTAINER</code>: Starts one or more stopped containers.</li> <li><code>docker compose -f docker-compose.yaml up -d</code>: Starts all containers defined in a Docker Compose file.</li> <li><code>docker compose -f docker-compose.yaml down</code>: Stops all containers defined in a Docker Compose file.</li> </ul>"},{"location":"appendixes/installing-docker/#references","title":"References","text":"<ol> <li> <p>Docker commands \u21a9</p> </li> </ol>"},{"location":"appendixes/installing-jdk/","title":"Installing Java 17.0.9","text":"<p>Essential for the development and execution of this workshop is the Java Development Kit (JDK).[^1] The JDK includes several tools such as a compiler (<code>javac</code>), a virtual machine, a documentation generator (<code>JavaDoc</code>), monitoring tools (Visual VM), and so on[^2]. The code in this workshop uses JDK {jdk-version}.</p>"},{"location":"appendixes/installing-jdk/#installing-the-jdk","title":"Installing the JDK","text":"<p>To install the JDK 17.0.9, follows the instructions from Adoptium Installation to download and install the JDK for your platform.</p>"},{"location":"appendixes/installing-jdk/#for-mac-os-x","title":"For Mac OS X","text":"<p>There is also an easier way to download and install Java if you are on Mac OS X: using SDKMAN! (SDKMAN!) is a tool for managing parallel versions of multiple Software Development Kits (SDK) on most Unix-based systems. It provides a convenient Command Line Interface (CLI) and API for installing, switching, removing, and listing Candidates. Developers often need to manage parallel versions of different builds of SDKs in their environment and switch from one to another. Manually setting the <code>PATH</code> variable can become quickly painful. That\u2019s when SDKMAN! can help you.</p>"},{"location":"appendixes/installing-jdk/#installing-sdkman","title":"Installing SDKMAN!","text":"<p>Installing SDKMAN! is easy. On Bash and ZSH shells simply open a new terminal and enter:</p> <pre><code>$ curl -s \"https://get.sdkman.io\" | bash\n</code></pre> <p>Follow the instructions on-screen to complete installation.</p> <p>Listing Java Versions To install Java, we need to list the available versions on SDKMAN! using the list command. The result is a table of entries grouped by the vendor and sorted by version:</p> <pre><code>$ sdk list java\n</code></pre> <p>If you have any Java candidate installed, you should see installed in the Status column.  If you don\u2019t have any Java candidate installed, use SDKMAN! to install one or several.</p>"},{"location":"appendixes/installing-jdk/#installing-java-1709_1","title":"Installing Java 17.0.9","text":"<p>There are several different vendors of Java, and each vendor has its own distribution.  Most of these distributions are available on SDKMAN! and can easily be installed.  Let\u2019s install Temurin.</p> <p>To install Temurin 17.0.9, we copy its identifier (17.0.9-tem), which is the version from the table, and we add it as an argument in the install command:</p> <pre><code>$ sdk install java 17.0.9-tem\n\nDownloading: java 17.0.9-tem\nRepackaging Java 17.0.9-tem...\nInstalling: java 17.0.9-tem\nDone installing!\nDo you want java 17.0.9-tem to be set as default? (Y/n):\n</code></pre>"},{"location":"appendixes/installing-jdk/#for-linux","title":"For Linux","text":"<p>For Linux distributions, there are also packaged Java installations.</p> <p>For rpm-based systems (e.g., Fedora):</p> <pre><code>dnf install java-17.0.9-openjdk\n</code></pre> <p>For Debian-based distributions (e.g., Ubuntu):</p> <pre><code>$ apt-get install openjdk-17.0.9-jdk\n</code></pre>"},{"location":"appendixes/installing-jdk/#checking-for-java-installation","title":"Checking for Java Installation","text":"<p>Once the installation is complete, it is necessary to set the <code>JAVA_HOME</code> variable and the <code>$JAVA_HOME/bin</code> directory to the <code>PATH</code> environment variable.  Check that your system recognizes Java by entering <code>java -version</code> and the Java compiler with <code>javac -version</code>.</p> <pre><code>$ java -version\nopenjdk version \"17.0.5\" 2022-10-18\n$ javac -version\njavac 17.0.5\n</code></pre>"},{"location":"appendixes/installing-ollama/","title":"Installing LLAMA2","text":"<p>To install LLAMA 2, we will use ollama. If you use Windows, you can follow the instructions from ollama\u2019s official Docker image. Note that when using Docker, the model will be running in a container. Without tuning, it is quite slow.</p> <p>If you use Linux or Mac, download ollama from ollama\u2019s download page and follow the installation instructions. You can use Homebrew on Mac:</p> <pre><code>$ brew install ollama\n</code></pre> <p>Once installed, you should have access to the <code>ollama</code> command line tool:</p> <pre><code>$ ollama --version\nollama version is 0.1.17\n</code></pre> <p>Ollama can run multiple models, as you can see on ollama\u2019s library. We will use the <code>llama2</code> model, which is a GPT-2 model trained on a large corpus of text. For that, you first need to start the ollama server:</p> <pre><code>$ ollama serve \n</code></pre> <p>Then, pull the model using:</p> <pre><code>$ ollama pull llama2 \n</code></pre> Warning <p>The LLAMA2 model is quite large (&gt; 3.8GB). Make sure you have enough disk space.</p> <p>You can check the list of available models using:</p> <pre><code>$ ollama list\nNAME            ID              SIZE    \nllama2:latest   78e26419b446    3.8 GB  \n</code></pre> <p>Once pulled, we will be able to use it.</p>"},{"location":"appendixes/installing-wsl/","title":"Installing WSL","text":"<p>Windows Subsystem for Linux (WSL) lets developers run a GNU/Linux environment \u2013 including most command-line tools, utilities, and applications \u2013 directly on Windows, unmodified, without the overhead of a traditional virtual machine or dual-boot setup.</p> Warning <p>If you are using Windows, it is recommended to install WSL as all the commands use bash.</p> <p>You can install everything you need to run Windows Subsystem for Linux (WSL) by entering this command in an administrator PowerShell or Windows Command Prompt and then restarting your machine:</p> <pre><code>wsl --install\n</code></pre> <p>This command will enable the required optional components, download the latest Linux kernel, set WSL 2 as your default, and install a Linux distribution for you (Ubuntu by default).</p> <p>The first time you launch a newly installed Linux distribution, a console window will open and you\u2019ll be asked to wait for files to de-compress and be stored on your machine. All future launches should take less than a second.</p>"},{"location":"workshop/01-slot1/","title":"Slot 1: Unveiling the Potential - Integrating OpenAI with Quarkus","text":"<p>In this first slot, we explore the potential of large language models like GPT-3/GPT-4 and how they integrate into Quarkus applications.  We walk through the process of integration, discussing the benefits and challenges of incorporating large language models into applications and explaining how Quarkus integration handles these challenges.</p>"},{"location":"workshop/01-slot1/#interacting-with-an-llm","title":"Interacting with an LLM","text":"<p>LLM stands for Large Language Model. However, you do not interact with them directly, you use a chat model.</p> <p>It follows a conversational pattern: you send a prompt, and the model responds with a message. The conversation is a sequence of messages and responses.</p> <p></p> Conversation with an LLM <p>The LLM answers to the user request and can use the context of the conversation to provide a more accurate response (.i.e. the set of already exchanged messages from the conversation).</p> <p>When integrating an LLM into an application, you need to define the prompt and the expected response. The prompt is a message sent to the LLM, and the response is the message returned by the LLM.</p> <p>In this slot, we will see how you can easily model your interaction with the LLM with Quarkus and the Quarkus LangChain4J extension. We are going to use Azure OpenAI as an example, but Quarkus can interact with many other LLMs.</p>"},{"location":"workshop/01-slot1/#the-triage-application","title":"The triage application","text":"<p>In this slot, we look at the code in the <code>triage-application</code> directory.  This application receives user reviews and classifies them as positive or negative. It\u2019s a simple application but serves as a good example of integrating OpenAI with Quarkus. In a terminal, navigate to the <code>triage-application</code> directory and run the following command:</p> <pre><code>$ export AZURE_OPENAI_API_KEY=&lt;your OpenAI API key&gt;\n$ ./mvnw quarkus:dev\n</code></pre> Info <p>The API key is used to access the Azure OpenAI service. Make sure you got it following the instructions in the appendix.</p> <p>This starts the application in development mode. Open a browser and navigate to http://localhost:8080 to see the application\u2019s frontend. The frontend is a simple form allowing you to submit a review. Upon submission, the application classifies it as positive or negative and displays the result.</p>"},{"location":"workshop/01-slot1/#anatomy-of-the-application","title":"Anatomy of the application","text":"<p>Let\u2019s delve into the code. Firstly, let\u2019s explore the TriageService interface:</p> <pre><code>package io.quarkiverse.langchain4j.workshop.triage;\n\nimport dev.langchain4j.service.SystemMessage;\nimport dev.langchain4j.service.UserMessage;\nimport io.quarkiverse.langchain4j.RegisterAiService;\n\n@RegisterAiService\npublic interface TriageService {\n\n    @SystemMessage(\"\"\"\n            You are working for a bank. You are an AI processing reviews about financial products. You need to triage the reviews into positive and negative ones.\n            You will always answer with a JSON document, and only this JSON document.\n            \"\"\")\n    @UserMessage(\"\"\"\n            Your task is to process the review delimited by ---.\n            Apply a sentiment analysis to the passed review to determine if it is positive or negative.\n            The review can be in any language. So, you will need to identify the language.\n\n            For example:\n            - \"I love your bank, you are the best!\", this is a 'POSITIVE' review\n            - \"J'adore votre banque\", this is a 'POSITIVE' review\n            - \"I hate your bank, you are the worst!\", this is a 'NEGATIVE' review\n\n             Answer with a JSON document containing:\n            - the 'evaluation' key set to 'POSITIVE' if the review is positive, 'NEGATIVE' otherwise, depending if the review is positive or negative\n            - the 'message' key set to a message thanking the customer in the case of a positive review, or an apology and a note that the bank is going to contact the customer in the case of a negative review. These messages must be polite and use the same language as the passed review.\n\n            ---\n            {review}\n            ---\n            \"\"\")\n    TriagedReview triage(String review);\n\n}\n</code></pre> <p>This interface defines the contract between the application and the AI service.  It contains the prompt that will be sent to the AI service and the expected response.</p> <p>Quarkus does not use a specific client.  It proposes using an interface and a set of annotations to model and invoke the AI service.  It encapsulates the complexity of the AI service, allowing you to focus on the business logic.</p> <p>The <code>@RegisterAiService</code> annotation registers the service.  Quarkus uses this annotation to generate the client code (at build time) and inject the client into the application.</p> <p>The <code>@SystemMessage</code> and <code>@UserMessage</code> annotations define the messages (prompt) sent to the AI service.  <code>@SystemMessage</code> defines the scope, context, and goal of the AI service.  <code>@UserMessage</code> defines the message sent to the AI service.</p> <p>Note that the triage method receives a String as a parameter (the user review).  The prompt can reference the parameter using the {review} placeholder.</p> Tip <p>Under the hood, Quarkus uses <code>qute</code> as a template engine.</p> <p>The prompt explains to the AI service what it needs to do and what response it should return.  Thus, the triage method returns a <code>TriageReview</code>:</p> <pre><code>package io.quarkiverse.langchain4j.workshop.triage;\n\nimport com.fasterxml.jackson.annotation.JsonCreator;\n\npublic record TriagedReview(Evaluation evaluation, String message) {\n\n    @JsonCreator\n    public TriagedReview {\n    }\n\n}\n</code></pre> <p>The <code>TriageReview</code> is a simple record that contains the evaluation (positive or negative) and the message to send to the user.</p> <p>Now, let\u2019s see how we can use your AI service:</p> <pre><code>package io.quarkiverse.langchain4j.workshop.triage;\n\nimport jakarta.inject.Inject;\nimport jakarta.ws.rs.POST;\nimport jakarta.ws.rs.Path;\n\n@Path(\"/review\")\npublic class ReviewResource {\n\n    @Inject\n    TriageService triage;\n\n    record Review(String review) {\n    }\n\n    @POST\n    public TriagedReview triage(Review review) {\n        return triage.triage(review.review());\n    }\n\n}\n</code></pre> <p>The <code>ReviewResource</code> is a simple JAX-RS resource that receives a <code>Review</code> and delegates the triage to the <code>TriageService</code>.  The <code>triage</code> method is called from the frontend you saw earlier.  The <code>TriageService</code> is injected by Quarkus and exposes the interface defined earlier.  So, we do not leak any details about the AI service.</p>"},{"location":"workshop/01-slot1/#configuring-the-ai-service","title":"Configuring the AI service","text":"<p>In the <code>application.properties</code> file, you will see the following (see instructions on how to set the <code>azure-openai</code> properties in the appendix):</p> <pre><code>quarkus.langchain4j.azure-openai.chat-model.temperature=0.5\nquarkus.langchain4j.azure-openai.timeout=60s\n\nquarkus.langchain4j.azure-openai.api-key=${AZURE_OPENAI_API_KEY}\nquarkus.langchain4j.azure-openai.deployment-id=${AI_DEPLOYMENT}\nquarkus.langchain4j.azure-openai.resource-name=${AI_SERVICE}\n</code></pre> <p>The first property, temperature, is used to control the creative aspect of the AI service.  The higher the temperature, the more creative the AI service will be.  In our case, we want to limit the creativity to avoid unexpected results.</p> <p>The second property is used to configure the timeout.  LLM can be slow to answer.  60s is generally a good value.  However, feel free to adapt.</p> <p>The last three properties configure the access to the Azure OpenAI service.</p> Logging the request and response <p>If you want to visualise the request that is sent to the model and its response, you cam increase the log level by adding the following properties to the <code>src/main/resources/application.properties</code> file:</p> <pre><code>quarkus.langchain4j.openai.log-requests=true\nquarkus.langchain4j.openai.log-responses=true\n</code></pre> Using plain OpenAI <p>You can also use plain OpenAI. You will need an OpenAI API key. Then, replace the content of the <code>application.properties</code> with:</p> <pre><code>quarkus.langchain4j.openai.api-key=${OPENAI_API_KEY}\nquarkus.langchain4j.openai.chat-model.temperature=0.5\nquarkus.langchain4j.openai.timeout=60s\n</code></pre> <p>Finally, in the <code>pom.xml</code>, replace <code>quarkus-langchain4j-azure-openai</code> with <code>quarkus-langchain4j-openai</code></p>"},{"location":"workshop/01-slot1/#under-the-hood","title":"Under the hood","text":"<p>As we have seen, Quarkus integrates LLM using a declarative approach.  It models the AI service using an interface and annotations. </p> <p>At build time, Quarkus generates the actual client that connects and invokes the remote model.  It uses langchain4j to manage that interaction.</p> <p>Note</p> <p>If you prefer a pure programmatic approach, you can use the langchain4j API directly in Quarkus.  However, you will lose some of the benefits we are going to see in the following sections.</p>"},{"location":"workshop/01-slot1/#fault-tolerance-and-resilience","title":"Fault-Tolerance and Resilience","text":"<p>In this section, we will explore how Quarkus can help you build fault-tolerant and resilient AI services.  First, check that the <code>pom.xml</code> file located in the <code>triage-application</code> directory contains the following dependency:</p> <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;io.quarkus&lt;/groupId&gt;\n    &lt;artifactId&gt;quarkus-smallrye-fault-tolerance&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre> <p>This Quarkus extension provides a set of annotations (as well as a programmatic API) to express the fault-tolerance and resilience requirements of your application.  Let\u2019s extend our AI service to make it more resilient.</p> <p>Open the <code>TriageService</code> interface and add (if not already present) the following annotation to the <code>triage</code> method:</p> <pre><code>// Do not forget to add the following imports:\n// import org.eclipse.microprofile.faulttolerance.Fallback;\n// import org.eclipse.microprofile.faulttolerance.Retry;\n\n@Retry(maxRetries = 2)\n@Fallback(fallbackMethod = \"fallback\")\n@RateLimit(value = 2, window = 10, windowUnit = ChronoUnit.SECONDS)\nTriagedReview triage(String review);\n</code></pre> <p>The <code>@Retry</code> annotation is used to retry the invocation of the AI service in case of failure.  In this case, we will retry twice.  The <code>@Fallback</code> annotation is used to define a fallback method that will be invoked if the AI service failed to answer (after the 2 retries).</p> <p>The <code>@RateLimit</code> annotation is used to limit the number of requests sent to the AI service. In this case, we will limit the number of requests to 2 per 10 seconds. Indeed, calling a AI service might be expensive. Also, for the workshop, the Azure OpenAI service has also a rate limit, thus we need to limit the number of requests. When the limit is reached, the fallback method is called.</p> <p>Thus,  let\u2019s implement the <code>fallback</code> method:</p> <pre><code>static TriagedReview fallback(String review) {\n    return new TriagedReview(Evaluation.NEGATIVE, \n        \"Sorry, we are unable to process your review at the moment. \" +\n        \"Please try again later.\");\n}\n</code></pre> <p>The <code>fallback</code> method returns a negative evaluation and a message explaining that the service is unavailable.  The Quarkus fault-tolerance support also provides timeout, circuit breaker and bulkhead. </p> <p>Check the Quarkus documentation for more details.</p>"},{"location":"workshop/01-slot1/#observability","title":"Observability","text":"<p>In this section, we will explore how Quarkus can help you monitor and observe your AI services. First, check that the pom.xml file located in the triage-application directory contains the following dependencies:</p> <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;io.quarkus&lt;/groupId&gt;\n    &lt;artifactId&gt;quarkus-micrometer-registry-prometheus&lt;/artifactId&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;io.quarkus&lt;/groupId&gt;\n    &lt;artifactId&gt;quarkus-opentelemetry&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre> <p>The first dependency is used to expose metrics using the Prometheus format. Quarkus metrics are based on Micrometer. The second dependency is used to expose traces using the OpenTelemetry format.</p>"},{"location":"workshop/01-slot1/#metrics","title":"Metrics","text":"<p>With the <code>quarkus-micrometer-registry-prometheus</code>, Quarkus will automatically expose metrics and traces for your application. It also provides specific metrics for the AI service.  For example, the number of requests, the number of errors, the response time, etc.</p> <p>Start the application and post a few reviews.  Then, open a browser and navigate to http://localhost:8080/q/metrics to see the metrics:</p> <pre><code># HELP langchain4j_aiservices_TriageService_triage_seconds\n# TYPE langchain4j_aiservices_TriageService_triage_seconds summary\nlangchain4j_aiservices_TriageService_triage_seconds_count 2.0\nlangchain4j_aiservices_TriageService_triage_seconds_sum 4.992278791\n# HELP langchain4j_aiservices_TriageService_triage_seconds_max\n# TYPE langchain4j_aiservices_TriageService_triage_seconds_max gauge\nlangchain4j_aiservices_TriageService_triage_seconds_max 2.706755083\n</code></pre> <p>You can see that the triage method has been invoked twice and that the response time sum was 4.992278791 seconds.  The max duration os the call was 2.706755083 seconds.</p>"},{"location":"workshop/01-slot1/#tracing","title":"Tracing","text":"<p>With the <code>quarkus-opentelemetry</code> extension, Quarkus will automatically expose traces for your application.  The Quarkus tracing support is based on OpenTelemetry.</p> <p>Before seeing traces, we must start the OpenTelemetry collector and the Jaeger UI (to visualize the traces).  In a new terminal, navigate to the <code>triage-application</code> directory and run the following command:</p> <pre><code>$ docker compose -f observability-stack.yml up\n</code></pre> <p>With the observability stack up, we can start submitting reviews for triage.  Then, open a browser and navigate to http://localhost:16686 to see the traces:</p> <ol> <li>Select the quarkus-llm-workshop-triage service</li> <li>Click on the Find Traces button</li> </ol> <p>You will see the trace on the right side of the screen.  If you click on one, you will see the details of the trace.  Quarkus instruments the AI service and the application to provide a complete trace.  Under that trace, you will see the trace of the actual call to the model (<code>POST</code>):</p> <p></p>"},{"location":"workshop/01-slot1/#summary","title":"Summary","text":"<p>This concludes the first slot.  In this slot, we have seen how Quarkus can help you integrate OpenAI with your application. Quarkus provides a declarative approach to integrate LLMs.  The interactions are modeled in a Java interface containing methods annotated with <code>@SystemMessage</code> and <code>@UserMessage</code>.  At build time, Quarkus generates the actual client code. </p> <p>We have also discussed how fault tolerance, metrics, and tracing have been added.</p>"},{"location":"workshop/02-slot2/","title":"Slot 2: Augmented Generation - Extending Hosted LLMs with Local Documents","text":"<p>In this section, we will:</p> <ol> <li>Shift gears: Explore another LLM and extend its capabilities using local documents.</li> <li>Retrieval Augmented Generation: Delve into one of the core patterns in AI usage, understanding the synergy between your data and LLM interactions.</li> </ol> <p></p> <p>We are going to build a chatbot that will use a pre-trained LLM (llama2) to generate responses. We will then extend the LLM with local documents to improve the quality of the responses.</p>"},{"location":"workshop/02-slot2/#running-llama2-locally","title":"Running LLAMA2 locally","text":"<p>In the previous slot, we used the OpenAI GPT model. The model is running remotely. It can become a security concern if you want to use the model for sensitive data, especially when using local documents to extend the knowledge of the LLM. Thus, here, we will run the model locally.</p> <p>If for any reason, you are not able to run the model locally, you can use a hosted LLAMA2 model. Jump to the Using a hosted LLama 2 model section.</p>"},{"location":"workshop/02-slot2/#installing-llama2","title":"Installing LLAMA2","text":"<p>If not already done, install LLAMA2 use ollama following the instructions discribed in the appendix).</p> <p>Once installed with Ollama up and running, you should have access to the <code>ollama</code> command line tool:</p> <pre><code>$ ollama --version\nollama version is 0.1.17\n</code></pre> <p>Ollama can run multiple models, as you can see on ollama\u2019s library. We will use the <code>llama2</code> model, which is a GPT-2 model trained on a large corpus of text.</p> <p>Pull the model using:</p> <pre><code>$ ollama pull llama2 \n</code></pre> Warning <p>The LLAMA2 model is quite large (&gt; 3.8GB). Make sure you have enough disk space.</p> <p>Once pulled, we will be able to use it.</p>"},{"location":"workshop/02-slot2/#using-a-hosted-llama-2-model","title":"Using a hosted LLama 2 model","text":"<p>If you are not able to run the model locally, you can use a hosted LLAMA2 model. To do so, you need to:</p> <ul> <li>Define the Hugging Face API key provided in the companion document:</li> </ul> <pre><code>$ export HUGGINGFACE_API_KEY=&lt;Your HuggingFace token&gt;\n</code></pre> <ul> <li>Then, in the rest of the section, you will use the <code>chat-application-hugging-face</code> directory instead of the <code>chat-application</code> directory. It uses a hugging face inference point providing the LLama2 model.  The configuration and dependencies are slightly different from the ones described in the rest of the section.</li> </ul>"},{"location":"workshop/02-slot2/#the-chat-application","title":"The Chat Application","text":"<p>As mentioned above, we will build a chatbot. Navigate to the <code>chat-application</code> directory to see the full source code of the application.</p> <p>All the paths in the rest of this document are related to that directory.</p>"},{"location":"workshop/02-slot2/#using-llama2","title":"Using LLAMA2","text":"<p>Open the <code>pom.xml</code> file and check that you have the following dependencies:</p> <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;io.quarkus&lt;/groupId&gt;\n    &lt;artifactId&gt;quarkus-resteasy-reactive-jackson&lt;/artifactId&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;io.quarkus&lt;/groupId&gt;\n    &lt;artifactId&gt;quarkus-websockets&lt;/artifactId&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;io.quarkiverse.langchain4j&lt;/groupId&gt;\n    &lt;artifactId&gt;quarkus-langchain4j-ollama&lt;/artifactId&gt;\n    &lt;version&gt;${quarkus-langchain.version}&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <p>The first dependency is the RESTEasy Reactive Jackson extension. The second dependency is used to add the WebSocket support, as the UI will use the WebSocket protocol to communicate with the application.</p> <p>Finally, the last dependency adds support for Ollama.</p> <p>In the <code>src/main/resources/application.properties</code> file, we have the following configuration:</p> <pre><code>quarkus.langchain4j.ollama.chat-model.model-id=llama2\nquarkus.langchain4j.ollama.timeout=60s\n</code></pre> <p>The first property is used to specify the model to use. In our case, we will use the <code>llama2</code> model. The second property configures the timeout for the model.</p> Logging the request and response <p>If you want to visualise the request that is sent to the model and its response, you cam increase the log level by adding the following properties to the <code>src/main/resources/application.properties</code> file:</p> <pre><code>quarkus.langchain4j.ollama.log-requests=true\nquarkus.langchain4j.ollama.log-responses=true\n</code></pre> <p>Alright, it\u2019s time to implement the chatbot part.</p>"},{"location":"workshop/02-slot2/#implementing-the-chat-bot","title":"Implementing the Chat Bot","text":"<p>The chat bot is composed of two parts:</p> <ul> <li>the AI service, which will generate the response</li> <li>the WebSocket endpoint, which will handle communication with the UI</li> </ul>"},{"location":"workshop/02-slot2/#the-ai-service","title":"The AI Service","text":"<p>The AI service is located in the <code>io.quarkiverse.langchain4j.workshop.chat.ChatService</code> class:</p> <pre><code>package io.quarkiverse.langchain4j.workshop.chat;\n\nimport dev.langchain4j.service.MemoryId;\nimport dev.langchain4j.service.SystemMessage;\nimport dev.langchain4j.service.UserMessage;\nimport io.quarkiverse.langchain4j.RegisterAiService;\nimport jakarta.inject.Singleton;\n\n@RegisterAiService\n@Singleton\npublic interface ChatService {\n\n    @SystemMessage(\"&lt;&lt;SYS&gt;&gt;You are a chat bot answering customer requests about bank products.&lt;&lt;/SYS&gt;&gt;\")\n    @UserMessage(\"\"\"\n        Answer the customer request. The answer must be polite and relevant to the question.\n        When you don't know, respond that you don't know the answer, and the bank will contact the customer directly.\n\n        +++\n        {message}\n        +++\n        \"\"\")\n    String chat(@MemoryId Object session, String message);\n\n}\n</code></pre> <p>The <code>ChatService</code> interface is annotated with <code>@RegisterAiService</code> to indicate that it is an AI service. The <code>@Singleton</code> annotation is used to indicate that the service is a singleton. This is required when used from a WebSocket.</p> <p>It contains a single method, <code>chat</code>, which is annotated with <code>@SystemMessage</code> and <code>@UserMessage</code>. The system message is wrapped into <code>&lt;&lt;SYS&gt;&gt;</code> and <code>&lt;&lt;\\SYS&gt;&gt;</code>. This is a requirement from the LLAMA2 model.</p> <p>The user message is a template that will be used to generate the response. The <code>+++</code> and <code>+++</code> are used to delimit the message from the user. Also note the <code>{message}</code> placeholder. It is replaced with the user message received as a parameter.</p>"},{"location":"workshop/02-slot2/#handling-the-state-of-the-conversation","title":"Handling the State of the Conversation","text":"<p>When interacting with a chat bot, we do not want to lose the context of the conversation. However, the LLM does not store the context of the conversation; it is stateless.</p> <p>Thus, we need to send the context of the conversation to the LLM every time we send a message. The context is a set of messages exchanged between the user and the chat bot.</p> <p>As you may have noticed, the <code>chat</code> method also receives a <code>session</code> parameter (which will be the WebSocket connection). The parameter is annotated with <code>@MemoryId</code>, indicating that this object will be used to store the state of the conversation.</p> <p>We need to provide a CDI bean implementing the <code>ChatMemoryProvider</code> interface:</p> <pre><code>package io.quarkiverse.langchain4j.workshop.chat;\n\nimport dev.langchain4j.memory.ChatMemory;\nimport dev.langchain4j.memory.chat.ChatMemoryProvider;\nimport dev.langchain4j.memory.chat.MessageWindowChatMemory;\nimport jakarta.enterprise.context.ApplicationScoped;\n\nimport java.util.Map;\nimport java.util.concurrent.ConcurrentHashMap;\n\n@ApplicationScoped\npublic class ChatMemoryBean implements ChatMemoryProvider {\n\n    private final Map&lt;Object, ChatMemory&gt; memories = new ConcurrentHashMap&lt;&gt;();\n\n    @Override\n    public ChatMemory get(Object memoryId) {\n        return memories.computeIfAbsent(memoryId, id -&gt; MessageWindowChatMemory.builder()\n                .maxMessages(20)\n                .id(memoryId)\n                .build());\n    }\n\n    public void clear(Object session) {\n        memories.remove(session);\n    }\n}\n</code></pre> Info <p>Because we have a single <code>ChatMemoryProvider</code>, we do not have to configure anything. When you have multiple ones, you can configure the one to use with the <code>chatMemoryProvider</code> attribute of the <code>@RegisterAiService</code> annotation.</p> <p>For each memory id, we create and retrieve a <code>ChatMemory</code> object. This object is used to store the context of the conversation for that specific memory id. In the code above, we only store 20 messages. Note that the bigger this context, the slower the response time. Even 20 can be too much.</p> <p>The <code>clear</code> method is used to remove the memory when the WebSocket connection is closed. That\u2019s what we are going to see now.</p>"},{"location":"workshop/02-slot2/#the-websocket-endpoint","title":"The WebSocket Endpoint","text":"<p>The second part is the WebSocket endpoint:</p> <pre><code>package io.quarkiverse.langchain4j.workshop.chat;\n\nimport io.smallrye.mutiny.infrastructure.Infrastructure;\nimport jakarta.enterprise.context.control.ActivateRequestContext;\nimport jakarta.inject.Inject;\nimport jakarta.websocket.*;\nimport jakarta.websocket.server.ServerEndpoint;\n\nimport java.io.IOException;\n\n@ServerEndpoint(\"/chatbot\")\npublic class ChatBotWebSocket {\n\n    @Inject\n    ChatService chat;\n\n    @Inject\n    ChatMemoryBean chatMemoryBean;\n\n    @OnClose\n    void onClose(Session session) {\n        chatMemoryBean.clear(session);\n    }\n\n    @OnMessage\n    public void onMessage(String message, Session session) {\n        Infrastructure.getDefaultExecutor().execute(() -&gt; {\n            String response = chat.chat(session, message);\n            try {\n                session.getBasicRemote().sendText(response);\n            } catch (IOException e) {\n                throw new RuntimeException(e);\n            }\n        });\n\n    }\n\n}\n</code></pre> <p>It is annotated with <code>@ServerEndpoint</code> to indicate that it is a WebSocket endpoint. The endpoint is available at the <code>/chatbot</code> path, so you can connect to the WebSocket using <code>ws://localhost:8080/chatbot</code>.</p> Tip <p>You can check if the port 8080 is already used by another process with the command <code>lsof -i tcp:8080</code>.</p> <p>The <code>ChatBotWebSocket</code> bean receives the <code>ChatService</code> as well as the <code>ChatMemoryBean</code> bean. The <code>onClose</code> method is called when the WebSocket connection is closed. It is used to remove the memory associated with the session.</p> <p>The <code>onMessage</code> method is called when a message is received. It uses the <code>ChatService</code> to generate the response and sends it back to the client.</p> Bug <p>Due to a Quarkus WebSocket limitation, we need to use <code>Infrastructure.getDefaultExecutor().execute</code> to execute the code in a different thread; otherwise, the WebSocket connection will block the event loop.</p>"},{"location":"workshop/02-slot2/#the-frontend","title":"The Frontend","text":"<p>The frontend is located in the <code>src/main/resources/META-INF/resources/index.html</code> file. Nothing very fancy.</p> <p>Start the application using:</p> <pre><code>$ ./mvnw quarkus:dev\n</code></pre> <p>Then, open your browser at http://localhost:8080 and click on the chat bot link (bottom right). You can start chatting with the bot. If you ask questions about the products offered by the bank, the bot will answer. But how does it get this knowledge? That\u2019s what we are going to see next.</p>"},{"location":"workshop/02-slot2/#extending-the-llm-with-local-documents","title":"Extending the LLM with Local Documents","text":"<p>In this section, we will extend the LLM with local documents describing the bank products. This is a two-steps process:</p> <ol> <li>Ingest the documents into the vector database.</li> <li>Find the relevant document and attach them to the user message (sent to the LLM).</li> </ol> <p>The second step is called retrieval augmented generation (RAG).</p>"},{"location":"workshop/02-slot2/#ingesting-documents","title":"Ingesting Documents","text":"<p>The first step is to ingest the documents into the vector database. The vector database is a database used to store the documents and their vector representation. Vectors allow semantic querying of the documents, for example, to find semantically relevant documents.</p> <p>To the ingestion consists of reading documents and computing a vector representation for each of them. This representation is called an embedding. Then, the vector and the document are stored into the vector database.</p> <p>In this application, we use Redis as a vector database. The <code>pom.xml</code> file contains the following dependency:</p> <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;io.quarkiverse.langchain4j&lt;/groupId&gt;\n    &lt;artifactId&gt;quarkus-langchain4j-redis&lt;/artifactId&gt;\n    &lt;version&gt;${quarkus-langchain.version}&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> Info <p>Quarkus also supports Chroma and PostgreSQL as a vector database.</p> <p>The ingestion process is implemented in the <code>DocumentIngestor</code> class:</p> <pre><code>package io.quarkiverse.langchain4j.workshop.chat;\n\nimport dev.langchain4j.data.document.Document;\nimport dev.langchain4j.data.document.loader.FileSystemDocumentLoader;\nimport dev.langchain4j.data.document.parser.TextDocumentParser;\nimport dev.langchain4j.model.embedding.EmbeddingModel;\nimport dev.langchain4j.store.embedding.EmbeddingStoreIngestor;\nimport io.quarkiverse.langchain4j.redis.RedisEmbeddingStore;\nimport io.quarkus.runtime.StartupEvent;\nimport jakarta.enterprise.context.ApplicationScoped;\nimport jakarta.enterprise.event.Observes;\nimport jakarta.inject.Inject;\n\nimport java.io.File;\nimport java.util.List;\n\nimport static dev.langchain4j.data.document.splitter.DocumentSplitters.recursive;\n\n@ApplicationScoped\npublic class DocumentIngestor {\n\n    /**\n     * The embedding store (the database).\n     * The bean is provided by the quarkus-langchain4j-redis extension.\n     */\n    @Inject\n    RedisEmbeddingStore store;\n\n    /**\n     * The embedding model (how the vector of a document is computed).\n     * The bean is provided by the LLM (like openai) extension.\n     */\n    @Inject\n    EmbeddingModel embeddingModel;\n\n    public void ingest(@Observes StartupEvent event) {\n        System.out.printf(\"Ingesting documents...%n\");\n        List&lt;Document&gt; documents \n            = FileSystemDocumentLoader.loadDocuments(new File(\"src/main/resources/catalog\").toPath(), new TextDocumentParser());\n        var ingestor = EmbeddingStoreIngestor.builder()\n                .embeddingStore(store)\n                .embeddingModel(embeddingModel)\n                .documentSplitter(recursive(500, 0))\n                .build();\n        ingestor.ingest(documents);\n        System.out.printf(\"Ingested %d documents.%n\", documents.size());\n    }\n}\n</code></pre> <p>The <code>ingest</code> method is called when the application starts. It uses the <code>FileSystemDocumentLoader</code> to load the documents from the <code>src/main/resources/catalog</code> directory. Then, it uses the <code>EmbeddingStoreIngestor</code> to ingest the documents into the vector database.</p> <p>The ingestor computes the embedding but also splits the document into smaller chunks. This is required to improve the performance (and reduce the size of the relevant data attached to the user request) of the retrieval process.</p> Tip <p>You do not have to use the embedding model provided by the LLM extension. You can also use a local model. It is recommended to use a local model when using a remote LLM to avoid having to send the full content to the remote LLM.</p> <p>In this example, we ingest documents during the application startup. However, it can be a dynamic process, ingesting documents on the fly. In general, the ingestion and the retrieval processes are decoupled into two different applications.</p>"},{"location":"workshop/02-slot2/#implementing-the-rag-pattern","title":"Implementing the RAG Pattern","text":"<p>Let\u2019s implement the second step. The RAG pattern is implemented in the <code>DocumentRetriever</code> class:</p> <pre><code>package io.quarkiverse.langchain4j.workshop.chat;\n\nimport dev.langchain4j.data.segment.TextSegment;\nimport dev.langchain4j.model.embedding.EmbeddingModel;\nimport dev.langchain4j.retriever.EmbeddingStoreRetriever;\nimport dev.langchain4j.retriever.Retriever;\nimport io.quarkiverse.langchain4j.redis.RedisEmbeddingStore;\nimport jakarta.enterprise.context.ApplicationScoped;\n\nimport java.util.List;\n\n@ApplicationScoped\npublic class DocumentRetriever implements Retriever&lt;TextSegment&gt; {\n\n    private final EmbeddingStoreRetriever retriever;\n\n    DocumentRetriever(RedisEmbeddingStore store, EmbeddingModel model) {\n        retriever = EmbeddingStoreRetriever.from(store, model, 5);\n    }\n\n    @Override\n    public List&lt;TextSegment&gt; findRelevant(String s) {\n        return retriever.findRelevant(s);\n    }\n}\n</code></pre> <p>This class is a bean implementing the <code>Retriever</code> interface. Because we have only one <code>Retriever</code> bean, we do not have to configure anything. When you have multiple ones, you can configure the one to use with the <code>retriever</code> attribute of the <code>@RegisterAiService</code> annotation.</p> <p>The retriever is configured with the vector database and the embedding model. Then, when the user sends a request, the <code>findRelevant</code> method is called to find all the semantically relevant chunks of data. The chunks are then attached to the user message and sent to the LLM.</p> <p>To find the relevant chunks, the retriever computes the vector representation of the user query and asks the database to provide the most relevant chunks.</p> <p>You do not have to do anything about the attachment of the chunks to the user message; it is done automatically by the LLM extension.</p>"},{"location":"workshop/02-slot2/#summary","title":"Summary","text":"<p>That concludes the second slot of the workshop. We have seen how to use a local (or hosted) LLM (LLAMA2) to build a chat bot. We have also looked into the ingestion and RAG patterns to extend the LLM with local documents.</p>"},{"location":"workshop/03-slot3/","title":"Slot 3: Deploying an LLM using OpenShift AI","text":"<p>Welcome to the third slot, where we delve into the process of deploying a Language Model (LLM) using OpenShift AI.  In the subsequent slot, we will leverage this deployed model to rework the functionality of the triage application introduced in the first slot.</p>"},{"location":"workshop/03-slot3/#deploying-a-model-on-openshift-ai","title":"Deploying a Model on OpenShift AI","text":"<p>OpenShift AI provides a streamlined solution for deploying and managing models on the OpenShift platform, offering robust model-serving capabilities.  In this demonstration, we\u2019ll employ OpenShift AI using Caikit+TGIS runtime to deploy and run of our LLM model.</p> <p>We are reusing an existing model.  OpenShift AI also proposes a tool suite to create, train and evaluate models.</p> Info <p>The OpenShift AI team is discussing an alternative approach to serve LLM based on vLLM. However, this approach is not yet available in OpenShift AI. </p>"},{"location":"workshop/03-slot3/#accessing-the-deployed-model","title":"Accessing the Deployed Model","text":"<p>Once successfully deployed, your model becomes accessible through a REST API.  OpenShift AI will furnish you with the URL, and depending on the desired action and the model\u2019s capabilities, append one of the following paths to the end of the inference endpoint URL:</p> <pre><code>:443/api/v1/task/server-streaming-text-generation\n:443/api/v1/task/text-generation\n</code></pre> <p>For the Quarkus integration, the latter endpoint is utilized. </p> <p>As depicted by these paths, the model serving platform employs the HTTPS port of your OpenShift router (typically port 443) to handle external API requests.</p> <p>Utilize these endpoints for making API requests to your deployed model.  A practical example using the curl command is illustrated below:</p> <pre><code>$ curl --json '{\n\"model_id\": \"&lt;model_name&gt;\",\n\"inputs\": \"&lt;query_text&gt;\"\n}' https://&lt;inference_endpoint_url&gt;:443/api/v1/task/text-generation\n</code></pre>"},{"location":"workshop/03-slot3/#summary","title":"Summary","text":"<p>This concludes our exploration of the third slot in the workshop.  We\u2019ve covered the steps involved in serving and invoking an LLM hosted on OpenShift AI.  In the upcoming and final slot, we will customize the triage application introduced in the first slot to incorporate the capabilities of the recently deployed Language Model.</p>"},{"location":"workshop/04-slot4/","title":"Slot 4: Utilizing a model deployed on OpenShift AI","text":"<p>Welcome to the final slot of the workshop! In this session, we\u2019ll leverage the model deployed in the previous slot to customize the triage application introduced in the first slot.</p> <p>Depending on the model that have been deployed, the functionality of the triage application will be different. The application located in the <code>triage-application-openshift-ai</code> is using a simplistic LLM (<code>flant5s-l</code>) and thus is adapted accordingly.</p> OpenShift AI with the Caikit+TGIS runtime <p>Strictly speaking, it\u2019s not just OpenShift AI but OpenShift AI with the Caikit+TGIS runtime that powers our model deployment.</p>"},{"location":"workshop/04-slot4/#using-the-openshift-ai-quarkus-integration","title":"Using the OpenShift AI Quarkus integration","text":"<p>To interact with a model running on OpenShift AI, we\u2019ll include a specific dependency in the <code>pom.xml</code> file located in <code>triage-application-openshift-ai</code>. This dependency is tailored for OpenShift AI and facilitates interaction with the model serving platform and its API:</p> <p><pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;io.quarkiverse.langchain4j&lt;/groupId&gt;\n    &lt;artifactId&gt;quarkus-langchain4j-openshift-ai&lt;/artifactId&gt;\n    &lt;version&gt;${quarkus-langchain.version}&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> This dependency is equivalent to the ones we have used in the previous slots, but it is specific to OpenShift AI. It will allow Quarkus LangChain4J to interact with the model serving platform and its API.</p> <p>Additionally, we need to specify the model serving platform\u2019s URL and the model ID in the <code>application.properties</code> file:</p> <pre><code>quarkus.langchain4j.openshift-ai.base-url=https://flant5s-l-predictor-ch2023.apps.cluster-hj2qv.dynamic.redhatworkshops.io:443/api\nquarkus.langchain4j.openshift-ai.chat-model.model-id=flant5s-l\n</code></pre> <p>For now, we are going to use the Flan T5 model model. This model is not as powerful as GPT 3.5. Thus, the application is sightly simplified.</p> Note <p>The <code>quarkus.tls.trust-all=true</code> property is included to bypass certificate validation, as the API uses HTTPS with self-signed certificates. This should not be done in production.</p>"},{"location":"workshop/04-slot4/#the-triage-service","title":"The triage service","text":"<p>The <code>TriageService.java</code> interface is the cornerstone of the application:</p> <pre><code>package io.quarkiverse.langchain4j.workshop.triage;\n\nimport dev.langchain4j.service.UserMessage;\nimport io.quarkiverse.langchain4j.RegisterAiService;\nimport io.smallrye.faulttolerance.api.RateLimit;\nimport org.eclipse.microprofile.faulttolerance.Fallback;\nimport org.eclipse.microprofile.faulttolerance.Retry;\n\nimport java.time.temporal.ChronoUnit;\n\n@RegisterAiService\npublic interface TriageService {\n\n    @UserMessage(\"\"\"\n            You are working for a bank. You are an AI processing reviews about financial products. You need to triage the reviews into positive and negative ones.\n            You must answer with `positive` or `negative` accordingly.\n\n            {review}\n\n            \"\"\")\n    @Retry(maxRetries = 2)\n    @Fallback(fallbackMethod = \"fallback\")\n    @RateLimit(value = 2, window = 10, windowUnit = ChronoUnit.SECONDS)\n    String triage(String review);\n\n    static String fallback(String review) {\n        return \"Sorry, we are unable to process your review at the moment. Please try again later.\";\n    }\n\n}\n</code></pre> <p>As you can see, it\u2019s a simplified version of the one used in the first slot:</p> <ol> <li>We use only a <code>@UserMessage</code> and no <code>@SystemMessage</code>, as the model runtime do not allow multiple messages for now.</li> <li>We do not ask the LLM to generate JSON, but just a <code>positive</code> or <code>negative</code> string.</li> <li>We do not ask the LLM to generate a message for the user</li> </ol> <p>Because, all the attendees are using the same endpoint, and we have limited resources, we keep the <code>@RateLimit</code> annotation to avoid overloading the model serving platform.</p>"},{"location":"workshop/04-slot4/#running-the-application","title":"Running the application","text":"<p>To run the application, you need to use the following command form the <code>triage-application-openshift-ai</code> directory:</p> <pre><code>mvn quarkus:dev\n</code></pre> <p>Then, open the browser at the following URL: http://localhost:8080, and start posting reviews. Each time to click on the submit button, it sends the review to the application, which invokes the computes the message and invokes the LLM model. Once received, the response is passed to the browser and displayed.</p>"},{"location":"workshop/04-slot4/#using-another-model","title":"Using another model","text":"<p>One of the great benefits of OpenShift AI is that it allows you to deploy multiple model. So, we can switch to the Mistral model.</p> <p>Open the <code>application.properties</code> file to comment the Flan T5 configuration, and uncomment the Mistral one:</p> <pre><code># Flan Small\n# quarkus.langchain4j.openshift-ai.base-url=https://flant5s-l-predictor-ch2023.apps.cluster-hj2qv.dynamic.redhatworkshops.io:443/api\n# quarkus.langchain4j.openshift-ai.chat-model.model-id=flant5s-l\n\n# Mistral 7B\nquarkus.langchain4j.openshift-ai.base-url=https://mistral7b-xl-predictor-ch2023.apps.cluster-hj2qv.dynamic.redhatworkshops.io:443/api\nquarkus.langchain4j.openshift-ai.chat-model.model-id=mistral7b-xl\nquarkus.langchain4j.openshift-ai.timeout=60s\n\n## The deployed model is using a self-signed certificate, so we need to trust it.\nquarkus.tls.trust-all=true\n</code></pre> <p>Then, the next time you submit a review, it will call the Mistral model. Note that it is a much more powerful model, so the response time is longer.</p>"},{"location":"workshop/04-slot4/#summary","title":"Summary","text":"<p>This concludes this slot and the workshop. In this slot we have seen how to use a model deployed on OpenShift AI with Quarkus LangChain4J. We used two models: Flan T5 and Mistral, both served by OpenShift AI.</p> <p>To summarize the workshop, we have seen how to:</p> <ul> <li>Integrate several LLMs in Quarkus applications (Azure OpenAI, (local) Llama, Llama on Hugging Face, Flan T5, and Mistral)</li> <li>We have used multiple LLM provides (Azure, Hugging Face, and OpenShift AI)</li> <li>We added tracing, metrics and auditing to our application to verify that it behaves correctly</li> <li>We used Quarkus fault tolerance to handle failures and rate limiting gracefully</li> <li>We have seen how to implement a RAG (retrieval augmented generation) pattern</li> </ul> <p>We hope you enjoy the workshop. We are eager to hear your positive and negative feedback and suggestions for improvements.</p>"}]}